{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling, Cleaning, and Transforming\n",
    "\n",
    "The goal of this workshop is to understand how to import a new dataset, gain simple insights from the dataset, and clean + prepare the dataset\n",
    "\n",
    "## Content\n",
    "\n",
    "1. Data Structures\n",
    "- What is a Series\n",
    "- What is a DataFrame\n",
    "2. Accessing DataFrames\n",
    "- Importing a CSV File\n",
    "- Using head() and tail()\n",
    "3. Additional Operations\n",
    "- Subsetting\n",
    "- Creating a New Column\n",
    "- Mathematical Operations\n",
    "- Group By\n",
    "- Merging DataFrames\n",
    "4. Dealing with Null Values\n",
    "- Eliminating Rows\n",
    "- Imputation\n",
    "5. Encoding\n",
    "- Variable Types: Continuous, Ordinal, and Categorical\n",
    "- Label Encoding\n",
    "- One-Hot Encoding\n",
    "6. Standardization\n",
    "- String Manipulations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Import Libraries\n",
    "Before we can learn about pandas (or any library) we need to read in the library. \\\n",
    "In python libraries are read in like this:\n",
    "```\n",
    "import library_name as some_abbreviation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q0 Import numpy and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Pandas Data Structures\n",
    "In pandas, there are two key data structures - a series and a dataframe.\n",
    "A series is a one-dimensional labeled array (or list) capable of holding any data type.\n",
    "A dataframe represents tabular data and is composed of columns, where each column is a series.\n",
    "\n",
    "You can read more about the basic data structures in Panda's documentation: https://pandas.pydata.org/docs/user_guide/dsintro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 Create a Series containing the first 4 Fibonacci numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 Create a Dataframe using the Series you just created\n",
    "To create a dataframe from a series, `mySeries`, simply call the `pandas.DataFrame` method. In the method, add a dictionary with the column name, `colName` as a key, and with your series being the value. \n",
    "\n",
    "`myDataframe = pd.DataFrame({'colName': mySeries})`\n",
    "\n",
    "This results in a dataframe called myDataframe, with a single column being `mySeries`, and the name of the column being `colName`. Hint: choose something more descriptive than `colName` for your columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Common Pandas Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data\n",
    "Typically, datasets you will find online or those that are provided for you will be in .csv format (Excel). \n",
    "\n",
    "To read-in and create a new data frame from a csv in pandas we use:\n",
    "\n",
    "```\n",
    "df = pd.read_csv('path or url to csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.1 Read in the space_titanic.csv dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the dataset\n",
    "`df.head()` will get the first 5 rows of your dataset\n",
    "\n",
    "`df.tail()` will get the last 5 rows of your dataset, try \n",
    "\n",
    "\n",
    "*Note:* the following two examples are *attributes*, thus, they don't end with `()`\n",
    "\n",
    "`df.shape` will return the dimensions of your dataset, so (x, y) would mean your dataset has x rows, and y columns\n",
    "\n",
    "`df.dtypes` will return the data type of each column\n",
    "\n",
    "`df.nunique()` will return the number of unique values of each column\n",
    "\n",
    "`df.isna().sum()` will return the number of NaN values in each column. A NaN is simply an undefined value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2 What is the data type of the `Transported` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.3 How many NaN values are there in the `Age` column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting the dataset\n",
    "Most of the times, you don't need your *whole* dataset. What if you want to access just 1 or 2 columns? Just the middle 50 rows? Just rows where the Passenger is an adult? This is where subsetting comes in!\n",
    "\n",
    "To get only a certain set of columns, say \"colName1\" and \"colName2\", do\n",
    "```\n",
    "df[[\"colName1\", \"colName2\"]]\n",
    "```\n",
    "\n",
    "To get only a certain set of rows based on a column condition, say the values in \"colName1\" need to be greater than 50, do\n",
    "```\n",
    "df[df[\"colName1\"] > 50]\n",
    "```\n",
    "\n",
    "We can also combine conditions, just like typical boolean logic.\n",
    "```\n",
    "df[(df[\"colName1\"] <= 50) & (df[\"colName2\"] == \"Red\")]\n",
    "```\n",
    "\n",
    "To get only a certain range of rows, use the .iloc[desired row indices] function. The following will return the rows from the 5th row, to the 10th row\n",
    "```\n",
    "df.iloc[range(5,10)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.4 Select all passengers that are VIP and spent less than 1000 in the Food Court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.5 How many passengers are from Mars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new column\n",
    "Sometimes the provided columns in your dataset don't capture all the information you need. We can create new columns and add them to our dataset to hold this information.\n",
    "\n",
    "There are many ways to create a new column. Setting a new column to a constant value, will create a column with all the values in that column equal to the constant value.\n",
    "```\n",
    "df[\"newColName\"] = 0\n",
    "```\n",
    "More usefully, you can perform an operation on an existing column to obtain new values for the new column. This will create a new column named `newColName` where all the values are the sum of the values in `colName1` and `colName2`\n",
    "```\n",
    "df[\"newColName\"] = df[\"colName1\"] + df[\"colName2\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.6 Create a column, `TotalExpense`, that represents a passenger's total spending on all amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Operations\n",
    "Typically for numerical data, you want to get basic statistics which describe the distribution of the data. Whether you want the max, mean, or other statistics, there are simple methods to help obtain these values. Here are just a few examples:\n",
    "\n",
    "`df.describe()`: Gets an assortment of different statistics of the data\n",
    "\n",
    "`df.max()`: Get the maximum value of the data\n",
    "\n",
    "`df.min()`: Get the mimumum value of the data\n",
    "\n",
    "`df.sum()`: Get the sum of the values in the data\n",
    "\n",
    "A more comprehensive walk-through of the available methods can be found [here](https://medium.com/@kasiarachuta/basic-statistics-in-pandas-dataframe-594208074f85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.7 What is the mean Age of all passengers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Pandas Functions\n",
    "\n",
    "### Groupby\n",
    "While finding aggregate statistics is useful, sometimes its helpful to find such statistics (like sum, mean) per \"group.\" For example, what if we wanted to find the mean age of all passengers *from Earth*? In this case, we would use the groupby function.\n",
    "\n",
    "First, we select all columns that are useful for our operation. In this case, we need the `HomePlanet` (the variable we are grouping by), as well as the `Age`, the variable we are performing the aggregate function on. In this case, the aggregate function is `.mean()`.\n",
    "\n",
    "```\n",
    "age_by_home_df = df[[\"HomePlanet\", \"Age\"]].groupby([\"HomePlanet\"]).mean()\n",
    "```\n",
    "\n",
    "### Sorting\n",
    "Sorting is typically used for numerical data as well. Say we wanted our dataset to be sorted from youngest to oldest passengers. We would use the sort_values function:\n",
    "\n",
    "```\n",
    "df.sort_values(by=[\"Age\"], ascending=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.8 What is the total amount spent on Room Service by all passengers from Europa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.8 What are the top five amounts passengers have spent on the Food Court?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Cleaning Dataset\n",
    "Cleaning your dataset helps solve two big issues in typical datasets:\n",
    "1. Null or NaN values\n",
    "2. Presence of Outliers\n",
    "\n",
    "Both issues can play a huge role in the performance of your model. That is why our primary objective in data cleaning is to ensure out dataset is clean of any null and outlier values.\n",
    "\n",
    "In this section, we'll focus on cleaning any null values\n",
    "\n",
    "## Handling Null values\n",
    "There are two primary ways of handling null values: **elimination** and **imputation**\n",
    "\n",
    "### Elimination\n",
    "There are two approaches to eliminating NaN: either delete rows, or columns. \n",
    "\n",
    "Typically we want to avoid deleting columns, as removing a column is removing a whole feature full of potentially useful information for our model. However, when the column has a great amount of data missing, sometimes it is the best decision to simply remove the column. To drop a column, use the `.drop` method, and specify the column names you'd like to drop, as well as the axis=1 which means column (axis = 0 is rows). If inplace is True, pandas will perform the drop on the current `df`, if False, pandas will return a new `df` with the column dropped.\n",
    "```\n",
    "df.drop(labels=[\"colName1\"], axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "Removing rows is typically safer, as long as we aren't removing too many rows. Remember, greater data can yield more precise insights. To remove rows where values in `colName1` are null:\n",
    "```\n",
    "df.dropna(subset=[\"colName1\"], inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1 Drop all rows where a passenger's HomePlanet is null. \n",
    "\n",
    "Check to see that your function worked by using `.isnull().sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation\n",
    "Imputation is the process of replacing null or problematic values in the dataset with more sensical values. \n",
    "\n",
    "For example, if you'd like to fill in the null values in `colName1`, one idea may be to fill in those values with the average of the other existing values. \n",
    "```\n",
    "[1, 2, NaN, 6] --> after average-based imputation, NaN would take the value of 3. \n",
    "```\n",
    "\n",
    "To impute, use the fillna() method. This example will fill ALL null values in the dataset with averages from their respective columns. \n",
    "```\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "```\n",
    "\n",
    "*Note:* Imputation needs to be performed with great care. As a data scientist, you don't want to let your decisions about imputations severely bias the resulting analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Transforming Dataset\n",
    "Transforming your dataset refers to the process of making your data more usable and understandble for your model and computer. \n",
    "\n",
    "For example, if you had a `color` column in your dataset, how would your model interpret `red`, `blue` `green`, etc.?\n",
    "\n",
    "Data, like `color`, that falls into categories is considered `categorical` data. All groups are of equal value. Sometimes there is no natural ordering (like `color`), other times, there is a natural ordering, like `letterGrade` in a class\n",
    "\n",
    "## Encoding\n",
    "Encoding is the process of making such data usable and interpretable for your model.\n",
    "\n",
    "Pandas' `get_dummies` method is very useful for converting unordered categorical data into numerical, computer-interpretable data. In this example, `get_dummies` will add columns (filled with 0s and 1s) corresponding to the different categories of `colName1`. Specifically, let's say a row had a value of `red` in `colName1`. After one-hot encoding, there will be a new column, `colName1_red` which is 1 if the row had value `red`, and 0 if the row had any other value. \n",
    "```\n",
    "df = pd.get_dummies(df, columns=[\"colName1\"])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.1 One-Hot encode the HomePlanet column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "Standardization refers to the process of transforming numerical data into a \"common unit\". \n",
    "\n",
    "For example, say you had a housing data, and one column was `listingPrice` and the other was `# of bedrooms`. If we don't standardize these two columns, our model will not interpret these as two different units. While a value like 800,000 is pretty normal for `listingPrice`, it is impossible for `# of bedrooms`. \n",
    "\n",
    "Standardization is also known as Z-score normalization. We are ensuring that each numerical column of our data has a mean of 0 and a standard deviation of 1, so that the columns (or features) of our data are more easily comparable to one another. \n",
    "\n",
    "First, we import StandardScaler from the scikit-learn library.\n",
    "```\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "```\n",
    "Now we can initialize an instance of the StandardScaler\n",
    "```\n",
    "scaler = StandardScaler()\n",
    "```\n",
    "Now we can call the `fit_transform` method on our scaler, passing in the numerical column we'd like to scale!\n",
    "```\n",
    "scaled_colName1 = scaler.fit_transform(df[\"colName1\"].values.reshape(-1,1))\n",
    "```\n",
    "*Note:* We need the `.values.reshape(-1,1)` in order to do some additional formatting on our column that scikit-learn requires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.2 Standardize the TotalExpense column we created earlier. Make sure to import and initialize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That wraps up DEEP Workshop #2!\n",
    "See you next week where you'll be able to apply the techniques discussed in today's workshop to your team's dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'target': ['A', 'C', 'R', 'K']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target\n",
       "0      A\n",
       "1      C\n",
       "2      R\n",
       "3      K"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(target):\n",
    "    if target in 'ABCDEFGH':\n",
    "        return \"HC\"\n",
    "    elif target in \"IJ\":\n",
    "        return \"BP\"\n",
    "    elif target == \"K\":\n",
    "        return \"GH\"\n",
    "    elif target in \"LMN\":\n",
    "        return 'RT'\n",
    "    elif target in 'OPQ':\n",
    "        return 'AT'\n",
    "    elif target in 'RST':\n",
    "        return 'MI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df['target'].apply(lambda x: categorize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>HC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "      <td>MI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K</td>\n",
       "      <td>GH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target category\n",
       "0      A       HC\n",
       "1      C       HC\n",
       "2      R       MI\n",
       "3      K       GH"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
