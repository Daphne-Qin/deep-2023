{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0: Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"space_titanic_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check label distribution\n",
    "print(str(data[data[\"Transported\"] == True].shape[0]) + \" passengers were Transported\")\n",
    "print(str(data[data[\"Transported\"] == False].shape[0]) + \" passeners were NOT transported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Processing + Feature Engineering\n",
    "Before we start building and training our model, we have to process our dataset a bit to get it ready for input to the model. More specifically, we should aim to do a few things:\n",
    "\n",
    "1. Remove unnecessary features\n",
    "2. Scale features\n",
    "3. Encode categorical data (categorical --> numerical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Unnecessary Features\n",
    "More often than not, there will be features we know will not help us predict our label. While doing so requires some domain knowledge, in this case, we will make some assumptions. We will remove the following features first:\n",
    "\n",
    "1. ```PassengerId```\n",
    "2. ```Name```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the PassengerId and Name columns from the dataset\n",
    "data = data.drop([\"PassengerId\", \"Name\"], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another time we may want to remove features, is if they would be too difficult to encode into numerical data. This might include long text data (reviews, comments), or categorical data with too many categories (high cardinality, different possible values). \n",
    "\n",
    "Let's take a look at the ```Cabin``` column. Getting all the unique values, we see that there are about 5305 different cabins. In this case, it might be best to remove the ```Cabin``` feature from out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check number of unique cabins\n",
    "data[\"Cabin\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Cabin column from dataset\n",
    "data = data.drop([\"Cabin\"], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (Encoding)\n",
    "Now that we've removed all the features we needed, let's move on to encoding our categorical variables. \n",
    "\n",
    "The most popular method is One-hot Encoding, and it is the one we will use. However, know that for different situations, we may want to use other methods of encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encode 'Destination' and 'HomePlanet' columns\n",
    "encoded = pd.get_dummies(data[[\"Destination\", \"HomePlanet\"]], drop_first=True, dtype=int)\n",
    "data.drop([\"Destination\", \"HomePlanet\"], axis=1, inplace=True)\n",
    "data = data.join(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing, we need to label encode our True/False columns. We'll denote 0 for False, and 1 for True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label encode CryoSleep, VIP, and Transported columns\n",
    "data['CryoSleep'] = data['CryoSleep'].map(lambda val: 1 if val else 0)\n",
    "data['VIP'] = data['VIP'].map(lambda val: 1 if val else 0)\n",
    "data['Transported'] = data['Transported'].map(lambda val: 1 if val else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset is all cleaned and processed, let's move onto modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Creating Training, Validation, and Testing Datasets\n",
    "\n",
    "The following command splits a given dataset into two distinct sets (training and testing): \n",
    "```\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "```\n",
    "* `X` is the set of feature variables in the dataset\n",
    "* `y` is the target variable in the dataset \n",
    "* `test_size` is the fraction of the original dataset that should be reserved for the testing\n",
    "\n",
    "**Note** Typically in machine learning we also perform what is called validation. Validation during training is like giving \"mini\" tests during the learning process. Typically validation helps with preventing the model from overfitting during training. For the sake of this workshop, we won't be covering validation but I encourage you to still read up about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, separate features from label\n",
    "X = data.drop(\"Transported\", axis=1, inplace=False) #every column except the label\n",
    "y = data[\"Transported\"] #just the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `X` and `y` to perform the `train_test_split` to obtain your training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing, it is always good practice to standardize our data before modeling. This ensures that all our numerical data is on the same scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize scaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit scaler to our training set\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use transform scale both the training and test set\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of training set: \" + str(X_train.shape[0]))\n",
    "print(\"Size of test set: \" + str(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to build our model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Modeling\n",
    "When modeling in machine learning, we rarely only use one model! In this case, we'll be trying a few different models:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Support Vector Classifier\n",
    "3. Decision Tree\n",
    "\n",
    "Note, there are many, many different models for classification. These are some of the more popular ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Let's start by creating a simple logistic regression model. We can import it and create an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit\n",
    "After we've created our model, we now need to train it using our training set. Use `.fit(X_train, y_train)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "Now that the model has been trained, we can test its performance using the test set. Use `.predict(X_test)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions from test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "There are many ways to evaluate how well our model performs. These methods vary between classification and regression as well. \n",
    "\n",
    "For classification, a very popular and intuitive metric is the accuracy, which is simply the percentage of observations that the model correctly classified (predicted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `accuracy_score(...)` to obtain the accuracy of our model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression achieved an accuracy of: \" + str(round(lr_accuracy *100, 2)) + \"%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, other popular metrics are the F1-Score, Recall and Precision. We can use a confusion matrix to obtain these values.\n",
    "\n",
    "Use `classification_report(...)` to obtain the confusion matrix and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "Besides evaluation, interpretation is also a key step. Given our model and its output, what can we deduce from it? For example:\n",
    "1. What are the most important features towards determining whether a passenger was transported or not?\n",
    "2. What are the least important features?\n",
    "\n",
    "This idea is called feature importance. Typically there's a trade-off between interpretability of a model and its complexity. \n",
    "\n",
    "Use your model's `.coef_[0]` attribute to get the feature coefficients. Put them alongside the name of the features (hint: use a dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the coefficient weights, we can tell that the most important feature (feature with the highest weight **magnitude**) are ```Spa```, ```VRDeck```, and ```HomePlanet_Europa```. Generally higher magnitude weights result in the weight having a stronger impact on the result of the prediction, whether it be negative or positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can do better with a more complex model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Validation with Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll do something similar as above, except we will introduce a few more concepts:\n",
    "1. Validation\n",
    "2. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "In addition to a training and test set, we typically use a validation set to test and choose the best hyperparameters for our model. \n",
    "\n",
    "Hyperparameters are sort of like 'settings' for our model. Each model has a variety of different hyperparameters that tune and affect the performance of the model. When validating hyperparameters, we normally instantiate lists of possible hyperparameter values, and iterate through each possible combination and testing the model accuracy. We keep track of the combination of hyperparameter values that yield the highest accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a train-test-split on the training set to get the validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter grid\n",
    "C = [0.01, 0.1, 0.25, 0.5, 0.75, 1.0]\n",
    "gamma = [0.01, 0.1, 0.15, 0.2, 0.3]\n",
    "kernels = [\"linear\", \"rbf\", \"poly\"]\n",
    "\n",
    "#keep track of best accuracy, best parameters\n",
    "best_svc = None\n",
    "best_val_acc = 0.0\n",
    "best_C, best_gamma, best_kernel = None,None,None\n",
    "\n",
    "#loop through each possible hyperparameter combination\n",
    "for kernel in kernels:\n",
    "    for c in C:\n",
    "        for gamm in gamma:\n",
    "            #create an SVC model using the hyperparameters\n",
    "            #fit the model\n",
    "            #get predictions from the validation set\n",
    "            #get the accuracy from the validation set (call it svc_accuracy)\n",
    "            # ---- type your answer here --- \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # --- type your answer here ---\n",
    "            \n",
    "            print(\"Accuracy: \" + str(round(svc_accuracy *100, 2)) + \"% with C=\" + str(c) + \", gamma=\" + str(gamm) + \" and kernel=\" + kernel)\n",
    "            \n",
    "            #check to see if accuracy improved\n",
    "            if svc_accuracy > best_val_acc:\n",
    "                best_svc = svc_model\n",
    "                best_val_acc = svc_accuracy\n",
    "                best_kernel = kernel\n",
    "                best_C = c\n",
    "                best_gamma = gamm\n",
    "                \n",
    "print(\"BEST Accuracy: \" + str(round(best_val_acc*100, 2)) + \"% with C=\" + str(best_C) + \", gamma=\" + str(best_gamma) + \" and kernel=\" + best_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well our **validated** model performs on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions from test set\n",
    "svc_pred = best_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get our model accuracy\n",
    "svc_test_accuracy = accuracy_score(y_test, svc_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVC achieved an accuracy of: \" + str(round(svc_test_accuracy*100, 2)) + \"%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "Your turn! Let's do everything one more time, now with the Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree Classsifier achieved an accuracy of: \" + str(round(dt_accuracy, 2)) + \"%!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's a wrap!\n",
    "There are **many** more types of models used for classification, as well as different metrics used to capture model performance. While we only went through a few, we encourage you to seek out more models on the Scikit-learn documentation!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
